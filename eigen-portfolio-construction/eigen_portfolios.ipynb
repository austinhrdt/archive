{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenportfolio Construction Using Principle Component Analysis\n",
    "By Austin Hardt on 20 March 2019.\n",
    "\n",
    "We will be constructing eigenportfolios using principle component analysis (PCA). The data I will be using is obtained from the IEX Finance api. Although there are other API's and data sources, I chose IEX because it is free and provides intraday data. Note that eigenportofolios are normally built using daily prices over years, not intraday prices over 10 days. This is an experiment.\n",
    "\n",
    "This notebook is lenghty so I have layed out the contents below. In short, what we will be learning is: how to collect, clean, and prepare real-time intraday data, how to construct eigen-portfoilios using principle components, and how to develop a brownian motion model to project future stock prices.\n",
    "\n",
    "\n",
    "## Contents  \n",
    "  \n",
    "**[1. Collect Data](#collectdata)**  \n",
    "&nbsp;&nbsp; [1.1 Retrieving Data From IEX](#retrieve)  \n",
    "&nbsp;&nbsp; [1.2 Obtaining & Storing Data](#obtain)  \n",
    "&nbsp;&nbsp; [1.3 Cleaning Data](#clean)  \n",
    "&nbsp;&nbsp; [1.4 Visualization](#visual)  \n",
    "  \n",
    "**[2. Preparing Data](#prepare)**  \n",
    "&nbsp;&nbsp; [2.1 Introduce an Index](#index)   \n",
    "&nbsp;&nbsp; [2.2 Compute Percent Returns](#returns)  \n",
    "&nbsp;&nbsp; [2.3 Splitting Data](#split)  \n",
    "  \n",
    "**[3. Eigen-Portfolio Construction](#eigen)**  \n",
    "&nbsp;&nbsp; [3.1 Principle Component Analysis](#pca)   \n",
    "&nbsp;&nbsp; [3.2 Define Sharpe Ratio](#sharpe)  \n",
    "&nbsp;&nbsp; [3.3 Building our Eigen-Portfolios](#building)  \n",
    "&nbsp;&nbsp; [3.4 Eigen-Portfolio Visualization](#viz)\n",
    " \n",
    " **[4. Projecting Stock Prices](#project)**   \n",
    "&nbsp;&nbsp; [4.1 Establish New Training Set](#establish)   \n",
    "&nbsp;&nbsp; [4.2 Develop Geometric Brownian Motion (GBM) Model](#gbm)  \n",
    "&nbsp;&nbsp; [4.3 Prepare our New Data](#new)  \n",
    "&nbsp;&nbsp; [4.4 Construct New Eigen-Portfolios](#construct)  \n",
    "&nbsp;&nbsp; [4.5 Visualize our New Portfolios](#vis)\n",
    "\n",
    "  \n",
    "    \n",
    "We begin, of course, by importing the various libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import BDay # B for buisiness, not birth\n",
    "from iexfinance.stocks import get_historical_intraday\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"collectdata\"> </a> \n",
    "# 1. Collect Data\n",
    " \n",
    "We must first collect our data. If you already have a data source or collection method that produces clean & accurate data, skip to Chapter 2.\n",
    "\n",
    "<a id=\"retrieve\"> </a> \n",
    "### 1.1 Retrieving Data From IEX\n",
    "I may have complicated the process of obtaining data from IEX, so if you decide to use a different api (e.g., quandl) or data source, skip to section 1.2. However, my code is written to produce a specific format of the data, which is neccessary for PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain intraday \n",
    "def get_intraday(ticker, date):\n",
    "    df = pd.DataFrame(get_historical_intraday(ticker, date))\n",
    "    if df.empty or df['marketAverage'].isnull().any().any() : return None\n",
    "    df['date'] = pd.to_datetime(df['date'] + ' ' + df['minute'])\n",
    "    return df.set_index('date')['marketAverage']\n",
    "\n",
    "# obtain data for multiple days\n",
    "def get_multiple_days(ticker, period):\n",
    "    intraday_data = []\n",
    "    for day in reversed(range(period)):\n",
    "        date = datetime.datetime.today() - BDay(day) - BDay(1) # TODO remove this extra day.\n",
    "        intraday_data.append(get_intraday(ticker, date))\n",
    "    df = pd.concat(intraday_data).rename(ticker)\n",
    "    return df[df != -1].dropna()\n",
    "\n",
    "# combine data for each stock into a dataframe\n",
    "def merge_data(period):\n",
    "    stocks, tickers = [], get_tickers()\n",
    "    for ticker in tqdm(tickers):\n",
    "        try: stocks.append(get_multiple_days(ticker, period))\n",
    "        except: pass\n",
    "    df = pd.concat(stocks, axis=1)\n",
    "    return df\n",
    "\n",
    "# returns a list of tickers.\n",
    "def get_tickers():\n",
    "    return list(pd.read_csv('tickers.csv')['Symbol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"obtain\"> </a> \n",
    "### 1.2 Obtaining & Storing Data\n",
    "I have written the above functions in such a way that I collect the entirety of my dataframe into a single variable. However, since im using IEX and not an already prepared data file, this takes quite a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = merge_data(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets store this dataframe as a csv file. This way (hopefully) we never have to run the above script again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy. Now we just read our csv, and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads csv\n",
    "def read(filename):\n",
    "    return pd.read_csv(filename, index_col='date') #make sure to specify the date is the index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read('data_test.csv')\n",
    "print('Dimensions of our data: ', df.shape)\n",
    "df.iloc[:5, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clean\"> </a> \n",
    "### 1.3 Cleaning Data\n",
    "\n",
    "Notice the NaNs. There are a good bit of them throughout the data. We must take care of them before we even think about building a model. In this case, I dropped all stocks that did not have at least 97% good data, then filled some others using a rolling mean. Note that this is a dangerous thing to do, so be very thorough if you use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans the data\n",
    "def clean_data(df):\n",
    "    df = df.dropna(axis=1, thresh = int(round(len(df.index) * 0.95))) # drops stocks without 95% good data.\n",
    "    df = df.fillna(df.rolling(window=10, min_periods=1).mean(), axis=1) # rolling mean to fill isolated NaNs.\n",
    "    df = df.dropna(axis=0)\n",
    "    return df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(df)\n",
    "print('New dimensions: ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we lost a decent amount of stocks, its important to not be lenient when filling NaNs. Adding more and more 'false' data will decrease the validity of our model.\n",
    "\n",
    "<a id=\"visual\"> </a> \n",
    "### 1.4 Visualization (Optional)\n",
    "\n",
    "Note that for most data science projects, visualizing the data is just as important as preparation or modelling. In fact, it should deserve its own chapter. However, since we all already know what stock market data looks like, and since there are tens of online tools that offer visualization better than that of matplotlib or seaborn, we wont spend much time on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1, axis=1).plot(figsize=(16,8), title=\"Price as a Function of Time for a Randomly Selected Stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare\"> </a> \n",
    "# 2. Preparing Data\n",
    "\n",
    "Now that we have collected our data and it is clean, we may begin preparing it for PCA.\n",
    "\n",
    "<a id=\"index\"> </a> \n",
    "### 2.1 Introduce an Index\n",
    "Lets sort our columns alphabetically and introduce an index. Since these companies are drawn from different exchanges, we will need an index in order to compare our eigenportfolios with the market. I defined it to be the average price of all companies combined. Note this is definitely not how market indexes are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort columns alphabetically\n",
    "def sort_data(df):\n",
    "    return df.reindex(sorted(df.columns), axis=1) \n",
    "\n",
    "# introduces market index\n",
    "def add_index(df):\n",
    "    df['INDEX'] = df.sum(axis=1) / len(df.columns) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_index(sort_data(df))\n",
    "print('Stock Prices Shape:', df.shape)\n",
    "df.iloc[-5:, -15:] # the index is found at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"returns\"> </a> \n",
    "### 2.2 Compute Percent Returns\n",
    "\n",
    "We first initialize two empty matrices of the same dimensions as our dataframe. Then, we calculate percent returns and standardize them. Note the first row of the data will be NaN, so we will ignore (remove) it.\n",
    "\n",
    "Now we standardize the percent returns. This is done by demeaning the returns and scaling by the standard deviation,\n",
    "\n",
    "$$ Z = \\frac{X - \\mu}{\\sigma},$$\n",
    "\n",
    "where $X$, $\\mu$, and $\\sigma$ are respectively the percent returns, mean, and standard deviation. We set $Z$, the standardized values, to fill our other empty matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes percent returns\n",
    "def compute_stock_returns(df):\n",
    "    stock_returns = pd.DataFrame(data = np.zeros(shape = (len(df.index), df.shape[1])),\n",
    "                                 columns = df.columns.values, index = df.index)\n",
    "    normed_returns = stock_returns\n",
    "    stock_returns = df.pct_change().dropna()\n",
    "    return (stock_returns, standardize(stock_returns))\n",
    "\n",
    "# standardizes the pct returns.\n",
    "def standardize(stock_returns):\n",
    "    return (stock_returns - stock_returns.mean()) / stock_returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[stock_returns, normed_returns] = compute_stock_returns(df)\n",
    "normed_returns.iloc[-5:, -10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split\"> </a> \n",
    "### 2.3 Splitting Data\n",
    "\n",
    "I chose to keep 90% of the data as the training set and the rest as the testing set. Note in finance the training set and testing set may be refered as 'in-sample' and 'out-of-sample'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine where to split\n",
    "def where_to_split(df):\n",
    "    split_where = int(df.shape[0] * 0.9)\n",
    "    here = df[split_where - 1:split_where]\n",
    "    return here.index[0]\n",
    "\n",
    "# split the data\n",
    "def split_data(df_returns):\n",
    "    train = df_returns[df_returns.index <= where_to_split(df_returns)].copy()\n",
    "    test = df_returns[df_returns.index > where_to_split(df_returns)].copy()\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train, df_test] = split_data(normed_returns)\n",
    "[df_raw_train, df_raw_test] = split_data(stock_returns)\n",
    "print('Train dataset:', df_train.shape)\n",
    "print('Test dataset:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the above results are reduced to one line\n",
    "[df_raw_train_tmp, df_raw_test_tmp] = split_data(compute_stock_returns(add_index(sort_data(clean_data(read('data.csv')))))[0])\n",
    "# check if we get the same result this way.\n",
    "if (df_raw_train_tmp.equals(df_raw_train) and df_raw_test_tmp.equals(df_raw_test)): print('Functions are Reproducible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eigen\"> </a> \n",
    "# 3. Eigen-Portfolio Construction\n",
    "\n",
    "Now that our data is prepared, we may begin constructing our eigen-portfolios.\n",
    "<a id=\"pca\"> </a> \n",
    "### 3.1 Principle Component Analysis\n",
    "We compute principle component analysis using our data. Then, we fix variance explained at some number and see what is the smallest number of components needed to explain this variance.\n",
    "\n",
    "We begin by computing the covariance matrix of our standarized training data. We then fit our model to the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes covariance matrix and fits pca model to it.\n",
    "def compute_pca(df_train):\n",
    "    pca = None\n",
    "    stock_tickers = df_train.columns.values[:-1]\n",
    "    cov_matrix = pd.DataFrame(data=np.ones(shape=(len(stock_tickers), len(stock_tickers))), columns = stock_tickers)\n",
    "    cov_matrix = df_train.loc[:, df_train.columns != 'INDEX'].cov()\n",
    "    return (PCA().fit(cov_matrix), cov_matrix)\n",
    "\n",
    "# prints how many components explain variance threshold, which I defined to be 80%\n",
    "def explain_variance(pca, var_thresh=0.8):\n",
    "    var_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "    num_comp = np.where(np.logical_not(var_explained < var_thresh))[0][0] + 1\n",
    "    print('{0} components explain {1}% of variance.'.format(num_comp, var_thresh * 100))\n",
    "\n",
    "\n",
    "# plots the variance explained    \n",
    "def plot_variance_explained(pca, cov_matrix):\n",
    "    n_asset = int((1/10) * cov_matrix.shape[1]) + 1\n",
    "    x_indx = np.arange(0, n_asset)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18, 6)\n",
    "    rects = ax.bar(x_indx, pca.explained_variance_ratio_[:n_asset], 1.2) # bar width = 1.2\n",
    "    ax.set_xticks(x_indx + 0.6) # xtick begins at end of bar\n",
    "    ax.set_xticklabels(list(range(n_asset)), rotation=66.6)\n",
    "    ax.set_title('Percent Variance Explained')\n",
    "    ax.legend((rects[0],), ('percent variance explained by principal components',))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pca, cov_matrix] = compute_pca(df_train)\n",
    "explain_variance(pca)\n",
    "plot_variance_explained(pca, cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we preform a fit-transform on the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_pca(pca, cov_matrix):\n",
    "    return pca.fit_transform(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected = fit_transform_pca(pca, cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sharpe\"> </a> \n",
    "### 3.2 Define Sharpe Ratio\n",
    "\n",
    "We define a sharpe metric in order to measure the preformance of our portfolios. The normal definition of the sharpe ratio describes preformance of annualized returns, but, in this experiment, we are using intraday data in a period of only ten days. Therefore, we will define a psuedo-sharpe-ratio to describe hourly preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(returns, periods=60): # working with minutely data, 60 min in hour.\n",
    "    n_hours = (returns.shape[0] / periods)\n",
    "    hourly_return = np.power(np.prod(1 + returns),(1 / n_hours)) - 1\n",
    "    hourly_vol = returns.std() * np.sqrt(periods)\n",
    "    return hourly_return / hourly_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building\"> </a> \n",
    "### 3.3 Building our Eigen-Portfolios\n",
    "\n",
    "We define an empty dictionary where we will store our portfolios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_portfolios = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The respective amounts invested in each stock, or the weights $w$, are defined as\n",
    "\n",
    "\n",
    "$$w_i^{(j)} = \\frac{v_i^{(j)}}{\\overline\\sigma_i},$$\n",
    "\n",
    "\n",
    "where $v$ is the set of all eigenvectors for our projection space. Specifically, $v$ are the principle components. Note that $j$ is the index of the eigen-portfolio and $i$ is representative of the stocks contained in $j$.\n",
    "\n",
    "The eigen-portfolio returns, $F$, for some duration of time, $k$, are therefore\n",
    "\n",
    "$$ F_{jk} = \\sum_{i}^{N} w_i^{(j)}R_{ik},$$\n",
    "\n",
    "where $R$ is stock $i$'s return data. \n",
    "\n",
    "Note that, normally, eigen portfolios contain negative weights (they still add up to 1). These negative weights pertain to shorts, where you'll need to borrow money. In this experiment, I negate negative weights and define my portfolios to consist of only 5 stocks. I am developing this program in the same sense as to why I am using intraday data: I want to know if these methods are viable for high frequency algorithmic trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds eigen-portfolios and appends them to our dictionary.\n",
    "# Note that in pca.components_, each row is a principle component\n",
    "# and that each column represents each feature.\n",
    "def construct_eigen_portfolios(eigen_portfolios, pca, df_raw_test, stock_tickers, n_portfolios=200):\n",
    "    sharpe_metric = np.array([0.] * n_portfolios)\n",
    "    for ix in range(n_portfolios):\n",
    "        eigen_portfolio = pd.DataFrame(data={'Weights': weights(pca.components_, ix).squeeze()*100}, index = stock_tickers)\n",
    "        eigen_portfolio.sort_values(by=['Weights'], ascending=False, inplace=True)\n",
    "        eigen_portfolio = adjust_weights(eigen_portfolio)\n",
    "        portfolio_stocks = eigen_portfolio['Weights'].index\n",
    "        raw_test = df_raw_test[portfolio_stocks.values]\n",
    "        eigen_returns = pd.Series(returns(eigen_portfolio, raw_test).squeeze(), index=raw_test.index)\n",
    "        sharpe_metric[ix] = sharpe_ratio(eigen_returns)\n",
    "        eigen_portfolios.update({'Portfolio {0}'.format(ix): {'Sharpe Ratio': sharpe_metric[ix],\n",
    "                                                              'Weights': eigen_portfolio['Weights'],\n",
    "                                                              'Returns': eigen_returns}})\n",
    "    return eigen_portfolios\n",
    "\n",
    "# computes weights for portfolio\n",
    "def weights(pcs, ix):\n",
    "    return pcs[:, ix] / sum(pcs[:, ix]) #normalize to 1\n",
    "\n",
    "# computes the returns for a given portfolio\n",
    "def returns(eigen_portfolio, df_raw_test):\n",
    "    return np.dot(df_raw_test.loc[:, eigen_portfolio.index], eigen_portfolio / 100)\n",
    "\n",
    "# removes negative weights\n",
    "def adjust_weights(eigen_portfolio):\n",
    "    adjusted_weights = eigen_portfolio['Weights']\n",
    "    adjusted_weights = adjusted_weights.nlargest(10) # I only want 5 stocks in my portfolio\n",
    "    adjusted_weights = adjusted_weights / adjusted_weights.sum() * 100\n",
    "    eigen_portfolio['Weights'] = adjusted_weights\n",
    "    return eigen_portfolio.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_portfolios = construct_eigen_portfolios(eigen_portfolios, pca, df_raw_test, df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"viz\"> </a> \n",
    "### 3.4 Eigen-Portfolio Visualization\n",
    "\n",
    "We now visualize our portfolios and gain insights as to which ones preform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect sharpe ratios for all portfolios\n",
    "def sharpes(eigen_portfolios):\n",
    "    sharpes = pd.DataFrame(eigen_portfolios).transpose()['Sharpe Ratio'].rename_axis('Portfolios')\n",
    "    return pd.DataFrame(sharpes.sort_values(ascending=False))\n",
    "\n",
    "# visualize distribution of shapre ratios for all portfolios\n",
    "def plot_sharpes(sharpes):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 6)\n",
    "    ax.plot(sharpes, linewidth=3)\n",
    "    ax.set_title('Sharpe Ratio of Eigen-Portfolios')\n",
    "    ax.set_ylabel('Sharpe ratio')\n",
    "    ax.set_xlabel('Portfolios')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Sharpe Ratio:', sharpes(eigen_portfolios).mean().values[0])\n",
    "plot_sharpes(sharpes(eigen_portfolios).sample(len(eigen_portfolios)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpes(eigen_portfolios).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sharpe ratios are quite bad. I should note that before I decided to select only 5 stocks for my portfolio (that is, when these portfolios contained negative weights), they preformed better and displayed higher sharpe ratios â€” as one could imagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpest_portfolio = sharpes(eigen_portfolios).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize portfolio weight distribution\n",
    "def plot_portfolio_weights(eigen_portfolios, portfolio):\n",
    "    stock_tickers = eigen_portfolios[portfolio]['Weights'].index\n",
    "    eigen_portfolios[portfolio]['Weights'].plot(title='{0} weights'.format(portfolio), figsize=(12,6), \n",
    "                      xticks=range(len(stock_tickers)), rot=45, linewidth=3)\n",
    "\n",
    "# compare portfolio preformance with market index\n",
    "def plot_portfolio_vs_index(eigen_portfolios, portfolio, df_raw_test):\n",
    "    df_plot = pd.DataFrame({'Portfolio': eigen_portfolios[portfolio]['Returns'],\n",
    "                                                          'INDEX': df_raw_test['INDEX']}, index=df_raw_test.index)\n",
    "    np.cumprod(df_plot + 1).plot(title='Returns of the market index vs. {0}'.format(portfolio),\n",
    "                                 figsize=(12,6), linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portfolio_weights(eigen_portfolios, sharpest_portfolio)\n",
    "plot_portfolio_vs_index(eigen_portfolios, sharpest_portfolio, df_raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"project\"> </a> \n",
    "# 4. Projecting Stock Prices\n",
    "\n",
    "We have learned how to construct eigen-portfolios. Now we will take all of the available data as the training set, and predict stock prices, utlizing a geometric brownian motion (GBM) model, to use as our testing set.\n",
    "\n",
    "<a id=\"establish\"> </a> \n",
    "### 4.1 Establish New Training Set\n",
    "\n",
    "We will take all of our previous data and use it as a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = add_index(sort_data(clean_data(read('data.csv'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy enough, now for the hard part.\n",
    "\n",
    "<a id=\"gbm\"> </a> \n",
    "### 4.2 Develop Geometric Brownian Motion (GBM) Model\n",
    "\n",
    "TODO: write up theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates new data\n",
    "def gen_data(assets):\n",
    "    tickers = list(assets.columns)\n",
    "    dates = get_dates(assets)\n",
    "    drift_vol = compute_drift_vol(assets)\n",
    "    predictions = pd.DataFrame(np.zeros([len(dates), assets.shape[1]]), index=dates, columns=assets.columns)\n",
    "    for ticker in tickers:\n",
    "        price_paths = simulate(ticker, assets, dates)\n",
    "        pred = pd.DataFrame(price_paths, columns=dates).sample(1).transpose() # taking one path\n",
    "        predictions[ticker] = pred  # filling our predictions dataframe\n",
    "    assets_preds = assets.append(predictions).rename_axis('date')\n",
    "    assets_preds.index = assets_preds.index.astype(str)\n",
    "    return assets_preds \n",
    "\n",
    "# simulates the model 25 times.\n",
    "def simulate(ticker, assets, dates, n_simulations=25):\n",
    "    drift_vol = compute_drift_vol(assets)\n",
    "    price_paths = []\n",
    "    for i in range(n_simulations):\n",
    "        [mu, v] = drift_vol[ticker]\n",
    "        price_path = generate_price_path(assets[ticker][-1], v, mu, dates)\n",
    "        price_paths.append(price_path)\n",
    "    return price_paths\n",
    "\n",
    "# computes predicted prices for all future dates.\n",
    "def generate_price_path(S, v, mu, dates):\n",
    "    price_path = []\n",
    "    for i in range(len(dates)):\n",
    "        pred_price = generate_asset_price(S, v, mu)\n",
    "        S = pred_price\n",
    "        price_path.append(pred_price)\n",
    "    return price_path\n",
    "\n",
    "# computes predicted price.\n",
    "def generate_asset_price(S,v,r,T=1/(30)):\n",
    "    return S * np.exp((r - 0.5 * v**2) * T + v * np.sqrt(T) * np.random.normal(0,1))\n",
    " \n",
    "# computes future dates (3 hours ahead of last known prices). \n",
    "def get_dates(assets):\n",
    "    start = pd.to_datetime(assets.index[-1]) + pd.Timedelta(minutes=1)\n",
    "    end = pd.to_datetime(assets.index[-1:]) + pd.Timedelta(minutes=30)\n",
    "    return pd.date_range(start=start, end=end[0], freq='min')\n",
    "\n",
    "# computes drift and volatility rates.\n",
    "def compute_drift_vol(assets, window=10):       \n",
    "    drift = ((assets[-window:].mean() - assets[:window].mean()) / assets[:window].mean()).rename('Drift') / 10\n",
    "    vol = assets.rolling(window=window, min_periods=1).mean()[-window:].std().rename('Vol') / 10\n",
    "\n",
    "    return pd.concat([drift, vol], axis=1).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Amazon predictions\n",
    "pred = pd.DataFrame(simulate('SU', assets, get_dates(assets)), columns=get_dates(assets)).transpose()\n",
    "pd.concat([assets['SU'], pred], axis=1).plot(figsize=(16, 3), legend=None)\n",
    "#plot Apple predictions\n",
    "pred = pd.DataFrame(simulate('WSM', assets, get_dates(assets)), columns=get_dates(assets)).transpose()\n",
    "pd.concat([assets['WSM'], pred], axis=1).plot(figsize=(16, 3), legend=None)\n",
    "#plot Nvidia\n",
    "pred = pd.DataFrame(simulate('YEXT', assets, get_dates(assets)), columns=get_dates(assets)).transpose()\n",
    "pd.concat([assets['YEXT'], pred], axis=1).plot(figsize=(16, 3), legend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "assets_preds = gen_data(assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"new\"> </a> \n",
    "### 4.3 Prepare our New Data\n",
    "\n",
    "\n",
    "Now we compute returns, split our data, and run principle component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[asset_returns, returns_normed] = compute_stock_returns(assets_preds)\n",
    "[train_raw, test_raw] = split_data(asset_returns)\n",
    "[train, test] = split_data(returns_normed)\n",
    "[pca, COV] = compute_pca(train)\n",
    "explain_variance(pca)\n",
    "plot_variance_explained(pca, COV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"construct\"> </a> \n",
    "### 4.4 Construct New Eigen-Portfolios\n",
    "\n",
    "Initialize empty dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our eigen portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = construct_eigen_portfolios(portfolios, pca, test_raw, test_raw.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vis\"> </a> \n",
    "### 4.5 Visualize our New Portfolios\n",
    "\n",
    "Lets see how these ones preformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Sharpe Ratio:', sharpes(portfolios).mean().values[0])\n",
    "plot_sharpes(sharpes(portfolios).sample(len(portfolios)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpes(portfolios).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, our portfolios outpreformed our previous ones. Could this be right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpest_portfolio = sharpes(portfolios).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portfolio_weights(portfolios, sharpest_portfolio)\n",
    "plot_portfolio_vs_index(portfolios, sharpest_portfolio, test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. We can practically see where our real data and GBM predicted data meet. It seems this method of eigen-portfolio construction exploits the GBM. Perhaps by reconstructing the GBM to be more random, we could achieve results that seem more realistic. Although, I do not think a GBM model should be utilized to project stock prices in this experiment. That being said, if one could approximate the drift and volatility coefficients in a precise manner, perhaps the model may be considered useful. \n",
    "\n",
    "Anyways, eigen-portfolio construction may be a viable method for portfolio selecting in the short term, but it will depend on market conditions, how much data you have, selection methods, and much more. In the long term, it is viable, and widely used. This program, with a few minor reconstructions, may be used to do just that. All you need is the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_portfolios(portfolios):\n",
    "    good_portfolios = {}\n",
    "    for i in range(len(portfolios)):\n",
    "        if portfolios['Portfolio {0}'.format(i)]['Sharpe Ratio'] > 2:\n",
    "            good_portfolios.update(portfolios['Portfolio {0}'.format(i)]['Weights'])\n",
    "    return good_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_portfolios = get_good_portfolios(eigen_portfolios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = []\n",
    "for i in range(len(eigen_portfolios)):\n",
    "    poop = list(eigen_portfolios['Portfolio {0}'.format(i)]['Weights'].index)\n",
    "    for poo in poop:\n",
    "        stuff.append(poo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(good_portfolios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(stuff).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_portfolios['Portfolio 95']['Returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.score(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
